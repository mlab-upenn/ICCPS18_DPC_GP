\section{Evolving Gaussian Processes}
\label{S:active}

In this section, we discuss the challenge of ``Model adaptability" listed in Sec.~\ref{SS:practical_challenges}.

As the system properties change with time, the learned model must actively update itself so that it best reflects the current behavior of the system. %  for best control performance.
For example, the same GP model may not be suitable to control a building in both Summer and Winter seasons.
As we generate more data with time with the controller in the loop, it is intuitive to incorporate the new data into the existing model to improve its accuracy.
However, we may not want to use the full new data set for model update for multiple reasons.
First, because not all data are created equal, especially in closed loop with a controller, we should select only the most informative subset of data that best explain the system dynamics at the time.
Second, since the computational complexity of training and predicting with Gaussian Processes is $\bigO(n^3)$, where $n$ is number of training samples, the learning and control problems become computationally hard as the size of data increases.
Therefore, obtaining the best GP model with the least amount data is highly desired.
The solution to this problem lies in \textit{selecting the optimal subset of data}, from the available data, that best explains the system behavior or dynamics.
Towards this goal, we extend the result from Sec.~\ref{SS:information-theory}.

\subsection{Optimal subset of data selection: selecting the most informative data for periodic model update}

Out goal is to filter the most informative subset of data that best explain the dynamics.
In this section, we outline a systematic procedure that aims to select the best $k$ samples from a given set $\D$ of $n$ observations.
The main differences between the problem of selecting the best or the most informative subset of data and the sequential sampling for OED described in Sec.~\ref{SS:oed:sequential} are that in the former, (1) all the features \(x\) must be optimized as opposed to only control variables \(u\), and (2) the decision has to be made only from the available data rather than sampling. 

We begin by selecting \(k\) samples randomly, %. This is the best result we can obtain using random sampling. We
then assign the priors of the hyperparameters \(\theta\) based on the MLE estimate obtained by learning a GP on the drawn set.
Starting with an empty set of samples \(\mathcal{S}\), %a set \(\mathcal{S}\) consisting of single sample,
we loop through the full data set \(\D\) to identify which sample maximizes the information gain. In this setup, we solve the following optimization problem
\begin{align}
\label{E:oed:batch}
\maximize_{x_{j}|(x_j,y_j) \in \mathcal{D} \setminus \mathcal{S}} & \ \ \ \tilde{\sigma}^2(x_j)/{\sigma}^2(x_j) 
\end{align}
Then, we add this sample to \(\mathcal{S}\), update \(\theta\) and proceed until \(|\mathcal{S}|=k\).
This algorithm is summarized in Algo.~\ref{A:oed:batch}. 

The proposed method is used in a case study in Sec.~\ref{SS:casestudy:active} to update the learned model from time to time as a controller runs in a closed loop and we generate more data.
Fig.~\ref{F:active:example} shows the improvement in mean prediction error and prediction variance obtained after optimal selection, starting with a model trained on uniformly random sampled data. %  using selection based on uniform sampling. 

\begin{algorithm}[!tb]
	\caption{Optimal subset of data selection}
	\label{A:oed:batch}
	\begin{algorithmic}[1]
		\Procedure{Initialization}{}
		\State Sample with replacement \(k\) integers \( \in \{1,\dots,n\} \)
		\State Compute \( \theta_{\mathrm{MLE}} = \argmax_{\theta^{\mathrm{MLE}}} \Pr(Y \vert X, \theta)\)
		\State Assign priors \(\theta_{\mathrm{0}} \sim \GaussianDist{\theta_{\mathrm{MLE}}}{\sigma^2_{\mathrm{init}}}\)
		\EndProcedure
		\State Define \(\mathcal{S} = \varnothing\)
		\Procedure{Sampling}{}
		\While{\( j \leq k \)}
		\State Solve \eqref{E:oed:batch} for optimal \({x_{j} \vert (x_j,y_j) \in \mathcal{D} \setminus \mathcal{S}} \)
		\State \(\mathcal{S} = \mathcal{S} \cup (x_j,y_j) \)
		\State Update \( \theta_{\mathrm{j}} = \argmax_{\theta^{\mathrm{MAP}}} \Pr(Y \vert X, \theta_{\mathrm{j-1}})\)
		\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{figure*}[t]
	\centering
	\setlength\fwidth{0.4\textwidth} 
	\setlength\hwidth{0.25\textwidth}
	\input{figures/batch-rand.tex} \hspace{0.5cm}
	\input{figures/batch-oed.tex}
	\caption{Left: Selection using random sampling. Right: Optimal subset of data selection. Starting with the random sampling, we use the model parameters and apply Algo.~\ref{A:oed:batch} to improve the model accuracy. Both the mean prediction error and the prediction variance are lower for optimal selection based on information gain. }
	\captionsetup{justification=centering}
	\label{F:active:example}
\end{figure*}


%\begin{figure}[!tb]
%	\centering
%	\setlength\fwidth{0.4\textwidth}
%	\setlength\hwidth{0.3\textwidth}	
%	\input{figures/batch-acc.tex}
%	\caption{Prediction of power consumption of a building. Top: 100 most informative samples are selected from available 1000 data points. Bottom: 100 samples are selected randomly. The mean prediction and variance in prediction are much better with active learning.}
%	\captionsetup{justification=centering}
%	\label{F:batch-acc}
%\end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
